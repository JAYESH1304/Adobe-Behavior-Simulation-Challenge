{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":7237794,"datasetId":4153541,"databundleVersionId":7327451},{"sourceType":"datasetVersion","sourceId":7237546,"datasetId":4191429,"databundleVersionId":7327203},{"sourceType":"datasetVersion","sourceId":7255519,"datasetId":4204338,"databundleVersionId":7345318},{"sourceType":"datasetVersion","sourceId":7255618,"datasetId":4195934,"databundleVersionId":7345417},{"sourceType":"datasetVersion","sourceId":7243330,"datasetId":4172681,"databundleVersionId":7333020}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install watchdog\n!pip install --no-dependencies --quiet streamlit\n!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n!unzip -o ngrok-stable-linux-amd64.zip\n!pip install --quiet pyngrok\n!pip install --no-dependencies --quiet protobuf==3.20.*   #==4.21.12\n!pip install --no-dependencies --quiet validators","metadata":{"execution":{"iopub.status.busy":"2023-12-21T15:35:29.806701Z","iopub.execute_input":"2023-12-21T15:35:29.807344Z","iopub.status.idle":"2023-12-21T15:36:04.273177Z","shell.execute_reply.started":"2023-12-21T15:35:29.807311Z","shell.execute_reply":"2023-12-21T15:36:04.271971Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting watchdog\n  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: watchdog\nSuccessfully installed watchdog-3.0.0\n--2023-12-21 15:35:46--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\nResolving bin.equinox.io (bin.equinox.io)... 54.161.241.46, 18.205.222.128, 52.202.168.65, ...\nConnecting to bin.equinox.io (bin.equinox.io)|54.161.241.46|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 13921656 (13M) [application/octet-stream]\nSaving to: 'ngrok-stable-linux-amd64.zip'\n\nngrok-stable-linux- 100%[===================>]  13.28M  56.8MB/s    in 0.2s    \n\n2023-12-21 15:35:47 (56.8 MB/s) - 'ngrok-stable-linux-amd64.zip' saved [13921656/13921656]\n\nArchive:  ngrok-stable-linux-amd64.zip\n  inflating: ngrok                   \n","output_type":"stream"}]},{"cell_type":"code","source":"!ngrok authtoken \"\" ","metadata":{"execution":{"iopub.status.busy":"2023-12-21T15:36:04.275085Z","iopub.execute_input":"2023-12-21T15:36:04.275396Z","iopub.status.idle":"2023-12-21T15:36:06.071686Z","shell.execute_reply.started":"2023-12-21T15:36:04.275369Z","shell.execute_reply":"2023-12-21T15:36:06.070728Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile my_app.py\nimport numpy as np \nimport pandas as pd \nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom catboost import CatBoostRegressor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nfrom transformers import DebertaTokenizer, DebertaModel\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nfrom torch.utils.data import DataLoader, Dataset\nimport torch\nimport torch.nn as nn\n\nimport requests\nfrom PIL import Image \nfrom io import BytesIO\nfrom tqdm import tqdm\nfrom IPython.display import Image as IPImage, display\n\nimport streamlit as st\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)\n\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\nDistil_bert = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\nDistil_bert.classifier = nn.Sequential(\n    nn.Linear(768, 5),\n    nn.Softmax(dim=1)\n)\nDistil_bert.load_state_dict(torch.load('/kaggle/input/nityam-model-1/log_model_state_dict.pth'))\nDistil_bert.eval()\n\n# Initialize BLIP processor and model\nblip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nblip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n# Load DeBERTa model and tokenizer\ndeberta_tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\ndeberta_model = DebertaModel.from_pretrained('microsoft/deberta-base')\n\n\nclass Test_Dataset(Dataset):\n    def __init__(self, Comments_):\n        self.comments = Comments_.copy()\n        self.comments[\"text\"] = self.comments[\"text\"].map(lambda x: tokenizer(x, padding=\"max_length\", truncation=True, return_tensors=\"pt\"))\n    \n    def __len__(self):\n        return len(self.comments)\n    \n    def __getitem__(self, idx):\n        comment = self.comments.loc[idx, \"text\"]\n        return comment\n\n    \ndef infer(model, Test_DL):\n    # Ensure model is in evaluation mode\n    model.eval()\n    pred = []\n    logs = []\n\n    with torch.no_grad():\n        for comments in Test_DL:\n            masks = comments[\"attention_mask\"].squeeze(1).to(device)\n            input_ids = comments[\"input_ids\"].squeeze(1).to(device)\n\n            # Move model to the same device as input tensors\n            model.to(device)\n\n            # Perform inference\n            output = model(input_ids, attention_mask=masks)\n\n            # Move logits and model components to CPU\n            logits = output.logits.cpu().numpy()  # Move logits to CPU and convert to NumPy\n            model.to('cpu')  # Move the model back to CPU for consistency\n\n            logs.append(logits)\n            pred_class = torch.argmax(output.logits.cpu(), dim=1).item()  # Move prediction to CPU\n            pred.append(pred_class)\n\n    return pred, logs\n\n\ndef fetch_image(image_url):\n\n    response = requests.get(image_url)\n    image = None\n    if(response.status_code == 200):\n        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n    else:\n        black_image_size = (224, 224)  \n        image = Image.new(\"RGB\", black_image_size, \"black\")\n    return image\n    \n    \ndef predict(final_text):\n    def clean(text):\n        text = text.replace(\"<hyperlink>\",\"\").replace(\"<mention>\",\"\")\n        return text\n    x = final_text\n    x = clean(x)\n\n    text_inputs = [x]\n    columns = ['text']\n    df = pd.DataFrame(text_inputs, columns=columns)\n\n    X_test = df\n    Test_data = Test_Dataset(X_test)\n    Test_Loader = DataLoader(Test_data, shuffle=False)\n    pred,logs = infer(Distil_bert,Test_Loader)\n\n    input_ids = deberta_tokenizer.encode(x, return_tensors='pt')\n    with torch.no_grad():\n        embeddings = deberta_model(input_ids).last_hidden_state\n\n    flattened_embeddings = embeddings.view(-1, embeddings.size(-1))\n    mean_embeddings = torch.mean(flattened_embeddings, dim=0)\n    mean_embeddings_np = mean_embeddings.numpy()\n\n    selected_bucket = pred[0]\n#     model_path = f\"/kaggle/input/regression-models/outputs/model_bucket_{selected_bucket}.joblib\"\n    model_path = f\"/kaggle/input/regression-models/outputs/outputs/model_bucket_{selected_bucket}.joblib\"\n    selected_model = XGBRegressor()\n    selected_model.load_model(model_path)\n    combined_features = mean_embeddings\n\n    features_reshaped = combined_features.view(1, -1).numpy()\n\n    prediction = selected_model.predict(features_reshaped)\n#     k = 15*np.exp(prediction/25)\n#     if(k > 5000):\n#         k = 150*prediction\n    return prediction\n\ndef main():\n    \n    st.title(\"Task1 : Behaviour Simulation\")\n\n\n    with st.form(\"user_input_form\"):\n\n        img_url = st.text_input(\"Enter the Image URL:\")\n        if img_url:\n            img = fetch_image(img_url)\n        else:\n            img = None\n            \n        if img is not None:\n            img = img.resize((224, 224))\n            st.image(img, caption=\"Processed Image\", use_column_width=True)\n        else:\n            black_image_size = (224, 224)  \n            img = Image.new(\"RGB\", black_image_size, \"black\")\n\n        tweet_content = st.text_input(\"Enter the Tweet Content: \")\n        inferred_company = st.text_input(\"Enter the Inferred Company: \")\n        date_time = st.text_input(\"Enter the Date and Time (e.g., 2018-01-29 10:51:17): \")\n\n        inputs = blip_processor(img, return_tensors=\"pt\")\n        out = blip_model.generate(**inputs)\n        image_caption = blip_processor.decode(out[0], skip_special_tokens=True)\n\n        final_text = (\n            f\"Following is the information about Twitter post.\"\n            f\"Caption for Image of post: {image_caption}, \"\n            f\"Text content: {tweet_content}, \"\n            f\"Inferred company: {inferred_company}, \"\n            f\"Date and time: {date_time} \"\n        )\n        \n        st.write(final_text)\n\n        submitted = st.form_submit_button(\"Submit\")\n        if submitted:\n            prediction = predict(final_text)\n            st.success(f\"Prediction : {prediction}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2023-12-21T15:36:06.073209Z","iopub.execute_input":"2023-12-21T15:36:06.073515Z","iopub.status.idle":"2023-12-21T15:36:06.084154Z","shell.execute_reply.started":"2023-12-21T15:36:06.073488Z","shell.execute_reply":"2023-12-21T15:36:06.083274Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Writing my_app.py\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyngrok import ngrok\nimport threading\n\ndef run_ngrok():\n    ngrok_tunnel = ngrok.connect(8501)\n    print(f'Public URL: {ngrok_tunnel.public_url}')\n    ngrok_tunnel.block_until_close()\n\n# Start ngrok in a separate thread\nngrok_thread = threading.Thread(target=run_ngrok)\nngrok_thread.start()","metadata":{"execution":{"iopub.status.busy":"2023-12-21T15:36:06.086546Z","iopub.execute_input":"2023-12-21T15:36:06.086968Z","iopub.status.idle":"2023-12-21T15:36:06.124998Z","shell.execute_reply.started":"2023-12-21T15:36:06.086936Z","shell.execute_reply":"2023-12-21T15:36:06.123463Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!streamlit run --server.port 8501 my_app.py > /dev/null","metadata":{"execution":{"iopub.status.busy":"2023-12-21T15:36:06.126066Z","iopub.execute_input":"2023-12-21T15:36:06.126395Z","iopub.status.idle":"2023-12-21T15:38:27.997097Z","shell.execute_reply.started":"2023-12-21T15:36:06.126364Z","shell.execute_reply":"2023-12-21T15:38:27.995997Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Exception in thread Thread-4 (run_ngrok):\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/tmp/ipykernel_42/567140438.py\", line 7, in run_ngrok\nAttributeError: 'NgrokTunnel' object has no attribute 'block_until_close'\n","output_type":"stream"},{"name":"stdout","text":"Public URL: https://6e53-35-237-118-44.ngrok-free.app\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\ntokenizer_config.json: 100%|██████████████████| 28.0/28.0 [00:00<00:00, 210kB/s]\nvocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 4.98MB/s]\ntokenizer.json: 100%|████████████████████████| 466k/466k [00:00<00:00, 8.93MB/s]\nconfig.json: 100%|█████████████████████████████| 483/483 [00:00<00:00, 4.08MB/s]\nmodel.safetensors: 100%|██████████████████████| 268M/268M [00:01<00:00, 255MB/s]\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\npreprocessor_config.json: 100%|████████████████| 287/287 [00:00<00:00, 2.05MB/s]\ntokenizer_config.json: 100%|███████████████████| 506/506 [00:00<00:00, 4.12MB/s]\nvocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 6.76MB/s]\ntokenizer.json: 100%|████████████████████████| 711k/711k [00:00<00:00, 12.8MB/s]\nspecial_tokens_map.json: 100%|█████████████████| 125/125 [00:00<00:00, 1.17MB/s]\nconfig.json: 100%|█████████████████████████| 4.56k/4.56k [00:00<00:00, 32.0MB/s]\npytorch_model.bin: 100%|██████████████████████| 990M/990M [00:03<00:00, 289MB/s]\ntokenizer_config.json: 100%|██████████████████| 52.0/52.0 [00:00<00:00, 303kB/s]\nvocab.json: 100%|████████████████████████████| 899k/899k [00:00<00:00, 23.4MB/s]\nmerges.txt: 100%|████████████████████████████| 456k/456k [00:00<00:00, 9.76MB/s]\nconfig.json: 100%|█████████████████████████████| 474/474 [00:00<00:00, 3.63MB/s]\npytorch_model.bin: 100%|█████████████████████| 559M/559M [00:09<00:00, 58.1MB/s]\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n^C\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}